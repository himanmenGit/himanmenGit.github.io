---
layout: post
title: "9.3 연습 문제"
categories:
- IntroducingPython
tags:
- IntroducingPython
---

# 9.3.1 webbrowser 모듈
```python
import anrtigravity
```
```python
import webbrowser
url = 'http://www.python.org'
webbrowser.open(url)
```
```python
webbrowser.open_new(url)
```
```python
webbrowser.open_new_tab(url)
```
# 9.3.2 웹API 와 REST
* 웹 페이지 내에서만 데이터가 사용되는 경우가 있다.
* 데이터에 접근하려면 웹 브라우저를 사용하여 페이지에 접속하고 데이터를 읽는다
* 웹 페이지 대신 애플리케이션 프로그래밍 인터페이스(API)를 통해 데이터를 제공 할 수 있다.
* 클라이언트는 URL 요청으로 서비스에 접근하여, 요청에 대한 상태와 데이터가 들어 있는 응답을 받을 수 있다.
* HTML페이지 대신 JSON과 XML같은 포맷을 사용하여 데이터를 쉽게 소비하는 프로그램을 작성할 수 있다.
RESTful 서비스는 다음의 특정 HTTP 동사를 사용한다.
* HEAD
> 실제 데이터가 아닌, 리소스에 대한 정보를 얻어온다.
* GET
> 이름에서 알 수 있듯이 GET은 서버에서 리소스의 데이터를 검색한다. GET은 브라우저에서 사용되는 표준 메서드다.
물픔요(?) 와 함계 인자들이 따라오는 URL이 바로 GET 요청이다. GET요청은 데이터를 생성, 변경, 삭제하는데 사용해서는 안된다.
* POST
> 이 동사는 서버의 데이터를 갱신한다. 주로 HTML 폼과 웹 API에서 사용한다.
* PUT
> 이 동사는 새로운 리소스를 생성한다.
* DELETE
> 이 동사는 서버의 데이터를 삭제 한다.
* 또한 RESTful 클라이언트는 HTTP 요청 헤더를 사용하여 서버로 부터 하나 이상의 콘텐트 타입을 요청 할 수 있다.
* 예를 들어 REST 인터페잇의 복합적인 서비스는 입력과 출력 형식으로 JSON 문자열을 선호한다.
# 9.3.3 JSON
* 웹 클라이언트와 서버간에 데이터를 교환하는데 유용하게 쓰인다.
# 9.3.4 크롤링과 스크래핑
1. 브라우저에 URL을 입력한다.
2. 원격 페이지가 불려올 때까지 기다린다.
3. 원하는 정보를 페이지를 통해서 본다.
4. 어딘가에 이 정보를 기록한다.
5. 다른 정보들도 URL을 입력하여 이 과정을 반복한다.
* 전체를 자동화 하는것이 좋다. 이것을 크롤러 혹은 스파이더라고 한다.
* 스크래퍼를 사용하여 정보를 파싱 한다.
```
pip install scrapy
pip install beautifulsoup4
```
* 웹페이지의 모든 링크 가져오기. HTML 의 a엘리먼트는 링크를 나타내고 href는 링크 목적지를 나타내는 속성이다.
* 링크를 얻기 위해 get_links() 함수를 정의 한다.

```python
def get_links(url):
import requests
from bs4 import BeautifulSoup as soup

result = requests.get(url)
page = result.text
doc = soup(page, 'lxml')
links = [element.get('href') for element in doc.find_all('a')]
return links
if __name__ == '__main__':
import sys
for url in sys.argv[1:]:
print('Links in', url)
for num ,link in enumerate(get_links(url), start=1):
print(num, link)
print()
```
